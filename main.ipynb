{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Main librairies\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Archives polytech\\DATA731 - Stochastique\\projet\\venv\\Lib\\site-packages\\matplotlib\\__init__.py:977\u001B[0m\n\u001B[0;32m    970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config\n\u001B[0;32m    973\u001B[0m \u001B[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001B[39;00m\n\u001B[0;32m    974\u001B[0m \u001B[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001B[39;00m\n\u001B[0;32m    975\u001B[0m \u001B[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001B[39;00m\n\u001B[0;32m    976\u001B[0m rcParamsDefault \u001B[38;5;241m=\u001B[39m _rc_params_in_file(\n\u001B[1;32m--> 977\u001B[0m     \u001B[43mcbook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data_path\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmatplotlibrc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    978\u001B[0m     \u001B[38;5;66;03m# Strip leading comment.\u001B[39;00m\n\u001B[0;32m    979\u001B[0m     transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m line: line[\u001B[38;5;241m1\u001B[39m:] \u001B[38;5;28;01mif\u001B[39;00m line\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m line,\n\u001B[0;32m    980\u001B[0m     fail_on_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28mdict\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(rcParamsDefault, rcsetup\u001B[38;5;241m.\u001B[39m_hardcoded_defaults)\n\u001B[0;32m    982\u001B[0m \u001B[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001B[39;00m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001B[39;00m\n\u001B[0;32m    986\u001B[0m \u001B[38;5;66;03m# fill in _auto_backend_sentinel.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Archives polytech\\DATA731 - Stochastique\\projet\\venv\\Lib\\site-packages\\matplotlib\\cbook.py:545\u001B[0m, in \u001B[0;36m_get_data_path\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_data_path\u001B[39m(\u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m    540\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    541\u001B[0m \u001B[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001B[39;00m\n\u001B[0;32m    542\u001B[0m \n\u001B[0;32m    543\u001B[0m \u001B[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001B[39;00m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Path(\u001B[43mmatplotlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data_path\u001B[49m(), \u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[1;32m~\\Desktop\\Archives polytech\\DATA731 - Stochastique\\projet\\venv\\Lib\\site-packages\\matplotlib\\_api\\__init__.py:217\u001B[0m, in \u001B[0;36mcaching_module_getattr.<locals>.__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m props:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m props[name]\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(instance)\n\u001B[1;32m--> 217\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "# Main librairies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "plt.rcParams['figure.figsize'] = [10,6]\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "# Other graph library\n",
    "# Heatmaps\n",
    "import seaborn as sns\n",
    "# Scores\n",
    "import graphviz\n",
    "\n",
    "# Preprocessing\n",
    "# Data Augmentation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Scores\n",
    "from scikitplot.metrics import plot_roc as auc_roc\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Models\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T17:24:55.323708400Z",
     "start_time": "2024-03-15T17:24:54.864497700Z"
    }
   },
   "id": "89e452adf168e25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Data Exploration\n",
    "df = pd.read_csv('spam.csv')\n",
    "target = 'spam'\n",
    "labels = ['Ham','Spam']\n",
    "features = [i for i in df.columns.values if i not in [target]]\n",
    "\n",
    "original_df = df.copy(deep=True)\n",
    "display(df.head())\n",
    "\n",
    "print(f'\\n\\033[1mInference:\\033[0m Le dataset contient {df.shape[1]} features et {df.shape[0]} samples')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyse des colonnes (types)\n",
    "df.info()\n",
    "# Nombre de valeurs uniques\n",
    "df.nunique().sort_values()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bad172c06474db7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check du nombre de valeurs uniques (on exclut \"spam\")\n",
    "nu = df[features].nunique().sort_values()\n",
    "\n",
    "# Features numérique (nf) et catégoriques (nc)\n",
    "nf = []; cf = []\n",
    "\n",
    "for i in range(df[features].shape[1]):\n",
    "    if nu.values[i]<=7:cf.append(nu.index[i])\n",
    "    else: nf.append(nu.index[i])\n",
    "print(f'\\n\\033[1mInference:\\033[0m Le dataset contient {len(nf)} features numériques et {len(cf)} features catégoriques.')\n",
    "\n",
    "# Transformation des features catégoriques en one-hot encoding\n",
    "df = pd.get_dummies(df, columns=cf, drop_first=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a99d383e8d553d7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Statistiques de chaque colonne\n",
    "display(df.describe())\n",
    "print(f'\\n\\033[1mInference:\\033[0m Les statistiques ont l\\'air corrects.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33a83994da4a05b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyse de la distribution de la cible\n",
    "df1 = df.copy()\n",
    "df1[target]=df1[target].map({1: \"Ham\", 0: \"Spam\"})\n",
    "print('\\033[1mDistribution variable de la cible'.center(55))\n",
    "plt.pie(\n",
    "    df1[target].value_counts(),\n",
    "    labels=df1[target].value_counts().index,\n",
    "    counterclock=False,\n",
    "    explode=[0, .1],\n",
    "    autopct='%1.1f%%',\n",
    "    radius=1,\n",
    "    startangle=0\n",
    ")\n",
    "plt.show()\n",
    "print(\"\\033[1mInference\\033[0m: La distribution de la cible semble in-équilibré. On va donc essayer de faire de la data augmentation.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e1d354316e5f148"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation de la matrice creuse\n",
    "plt.figure(figsize=(10,10))                                         \n",
    "starting_sample = random.randint(0,len(df)-100)\n",
    "plt.title(f'Matrice creuse des 100 premiers échantillons (index {starting_sample})')\n",
    "plt.spy(df[starting_sample:starting_sample+100].values, precision=.1, markersize = 5)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ab76a260cd10e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Suppression des duplicatas\n",
    "counter = 0\n",
    "\n",
    "# Nouveau dataset sans duplicatas (df1)\n",
    "df1 = df.copy()\n",
    "df1.drop_duplicates(inplace=True)\n",
    "df1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "if df1.shape==original_df.shape:\n",
    "    print('\\n\\033[1mInference:\\033[0m Le dataset ne contient pas de duplicatas')\n",
    "else:\n",
    "    print(f'\\n\\033[1mInference:\\033[0m {df.shape[0]-df1.shape[0]} duplicatas ont été supprimés')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4c68d1ab2bdd41c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Détection de sample vide\n",
    "nvc = pd.DataFrame(df1.isnull().sum().sort_values(), columns=['Nombre de null'])\n",
    "nvc['Percentage'] = round(nvc['Nombre de null']/df1.shape[0],3)*100\n",
    "display(nvc)\n",
    "print(\"\\n\\033[1mInference:\\033[0m Il n'a aucun sample vide\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f0f5feaf0a845d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Résolution de l'in-équilibrage de la cible en utilisant SMOTE\n",
    "\n",
    "# Nouveau dataset augmenté et sans duplicatas (df5)\n",
    "df5 = df1.copy()\n",
    "print('Distribution de la cible original')\n",
    "print(df5[target].value_counts())\n",
    "\n",
    "xf = df5.columns\n",
    "X = df5.drop([target],axis=1)\n",
    "Y = df5[target]\n",
    "\n",
    "smote = SMOTE()\n",
    "X, Y = smote.fit_resample(X, Y)\n",
    "\n",
    "df5 = pd.DataFrame(X, columns=xf)\n",
    "df5[target] = Y\n",
    "\n",
    "print('\\nDistribution de la cible post-SMOTE')\n",
    "print(Y.value_counts())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25583247ff68a8f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Taille du dataset final post-preprocessing\n",
    "plt.title('Échantillons finaux du dataset')\n",
    "plt.pie(\n",
    "    [df.shape[0], original_df.shape[0]-df1.shape[0], df5.shape[0]-df1.shape[0]], \n",
    "    radius = 1,\n",
    "    labels=['Originaux','Supprimés','Augmentés (SMOTE)'], \n",
    "    counterclock=False, \n",
    "    autopct='%1.1f%%', \n",
    "    pctdistance=0.9, \n",
    "    explode=[0,0,0]\n",
    ")\n",
    "plt.pie([df.shape[0]], labeldistance=-0, radius=0.78, colors=['powderblue'])\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n\\033[1mInference:\\033[0m Le dataset final après nettoyage contient {df5.shape[0]} samples.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cf4f28e945a9de3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Séparation de la donnée en test set et train set\n",
    "\n",
    "df = df5.copy()\n",
    "\n",
    "X = df.drop([target],axis=1)\n",
    "Y = df[target]\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "print('Dataset original | X', X.shape, '| Y', Y.shape)\n",
    "print('Training dataset | X', Train_X.shape, '| Y', Train_Y.shape)\n",
    "print('Testing dataset | X', Test_X.shape, '| Y', Test_Y.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f50ca56ee1011425"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Feature Scaling (Standardization)\n",
    "std = StandardScaler()\n",
    "\n",
    "Train_X_std = std.fit_transform(Train_X)\n",
    "Train_X_std = pd.DataFrame(Train_X_std, columns=X.columns)\n",
    "print('\\033[1mTraining set original'.center(100))\n",
    "display(Train_X.describe())\n",
    "print('\\033[1mTraining set normalisée'.center(100))\n",
    "display(Train_X_std.describe())\n",
    "\n",
    "Test_X_std = std.transform(Test_X)\n",
    "Test_X_std = pd.DataFrame(Test_X_std, columns=X.columns)\n",
    "print('\\033[1mTesting set original'.center(100))\n",
    "display(Test_X.describe())\n",
    "print('\\n','\\033[1mTesting set normalisée'.center(100))\n",
    "display(Test_X_std.describe())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc792aacef479107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation des correlations\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.title('Corrélation des caractéristiques')\n",
    "sns.heatmap(df[features].corr(), vmin=-1, vmax=1, center=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ed447b236be65f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test application du VIF pour réduire la multi colinéarité\n",
    "\n",
    "# DROP contient les features à ignorer\n",
    "# scores1, scores2 et scores3 contiennent respectivement les résultats des modèles de régression logistique, de random forest classifier, et de XGB classifier\n",
    "DROP=[]; scores1=[]; scores2=[]; scores3=[]\n",
    "scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "scores3.append(f1_score(Test_Y,XGBClassifier(eval_metric='logloss').fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "\n",
    "for i in range(len(X.columns.values)-1):\n",
    "    vif = pd.DataFrame()\n",
    "    Xs = X.drop(DROP,axis=1)\n",
    "    vif['Features'] = Xs.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(Xs.values, i) for i in range(Xs.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by=\"VIF\", ascending=False)\n",
    "    vif.reset_index(drop=True, inplace=True)\n",
    "    DROP.append(vif.Features[0])\n",
    "    \n",
    "    if vif.VIF[0]<=1: break\n",
    "    \n",
    "    scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "    scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "    scores3.append(f1_score(Test_Y,XGBClassifier(eval_metric='logloss').fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)),average='weighted')*100)\n",
    "    \n",
    "plt.plot(scores1, label='LR')\n",
    "plt.plot(scores2, label='RF')\n",
    "plt.plot(scores3, label='XG')\n",
    "plt.legend()\n",
    "plt.title(\"Variation du F1 score en supprimant séquentiellement les features de VIF maximum\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27494ca58e764623"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test application du Recursive Feature Elimination (RFE)\n",
    "LR = LogisticRegression()#.fit(Train_X_std, Train_Y)\n",
    "scores1=[]; scores2=[]; scores3=[]\n",
    "scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\n",
    "scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\n",
    "scores3.append(f1_score(Test_Y,XGBClassifier(eval_metric='logloss').fit(Train_X_std, Train_Y).predict(Test_X_std),average='weighted')*100)\n",
    "\n",
    "for i in range(len(X.columns.values)):\n",
    "    rfe = RFE(LR,n_features_to_select=len(Train_X_std.columns)-i)   \n",
    "    rfe = rfe.fit(Train_X_std, Train_Y)\n",
    "    scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n",
    "    scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n",
    "    scores3.append(f1_score(Test_Y,XGBClassifier(eval_metric='logloss').fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]]),average='weighted')*100)\n",
    "    \n",
    "plt.plot(scores1, label='LR')\n",
    "plt.plot(scores2, label='RF')\n",
    "plt.plot(scores3, label='XG')\n",
    "plt.legend()\n",
    "plt.title(\"Variation du F1 score avec suppression automatique des features par RFE\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd737479e199f5cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test application du Principal Components Analysis (PCA)\n",
    "pca = PCA().fit(Train_X_std)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,6))\n",
    "x_values = range(1, pca.n_components_+1)\n",
    "ax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Variance expliquée')\n",
    "ax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Variance expliquée cumulée', color='red')\n",
    "plt.plot([0,pca.n_components_+1],[0.90,0.90],'g--')\n",
    "plt.plot([43,43],[0,1], 'g--')\n",
    "ax.set_title('Variance expliquée des composants')\n",
    "ax.set_xlabel('Composant principal')\n",
    "ax.set_ylabel('Variance expliquée')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'\\n\\033[1mInference:\\033[0m On voit ici la variance expliquée (normalisée) cumulées en fonction du nombre de composants principaux')\n",
    "print(f'\\n\\033[1mInference:\\033[0m On observe que la variance expliquée cumulée atteint 0.9 à partir de 43 composants')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43df8ebf2e5b877d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test application de la feature extraction avec PCA\n",
    "\n",
    "scores1=[]; scores2=[]; scores3=[]\n",
    "for i in range(len(X.columns.values)):\n",
    "    pca = PCA(n_components=Train_X_std.shape[1]-i)\n",
    "    Train_X_std_pca = pca.fit_transform(Train_X_std)\n",
    "    Train_X_std_pca = pd.DataFrame(Train_X_std_pca)\n",
    "\n",
    "    Test_X_std_pca = pca.transform(Test_X_std)\n",
    "    Test_X_std_pca = pd.DataFrame(Test_X_std_pca)\n",
    "    \n",
    "    scores1.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca),average='weighted')*100)\n",
    "    scores2.append(f1_score(Test_Y,RandomForestClassifier().fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca),average='weighted')*100)\n",
    "    scores3.append(f1_score(Test_Y,XGBClassifier(eval_metric='logloss').fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca),average='weighted')*100)\n",
    "\n",
    "plt.plot(scores1, label='LR')\n",
    "plt.plot(scores2, label='RF')\n",
    "plt.plot(scores3, label='XG')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a21d75a3b5b8ffd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Application finale de la RFE (avec LR) pour 27 features (réduction de 30 features) \n",
    "LR = LogisticRegression()\n",
    "rfe = RFE(LR,n_features_to_select=27)\n",
    "Train_X_std_rfe = rfe.fit_transform(Train_X_std, Train_Y)\n",
    "print(f'Dimension finale du training dataset transformé : {Train_X_std_rfe.shape}')\n",
    "Train_X_std_rfe = pd.DataFrame(Train_X_std_rfe)\n",
    "\n",
    "Test_X_std_rfe = rfe.fit_transform(Test_X_std, Test_Y)\n",
    "print(f'Dimension finale du test dataset transformé : {Test_X_std_rfe.shape}')\n",
    "Test_X_std_rfe = pd.DataFrame(Test_X_std_rfe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15904540d8467bff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# On commence par créer une table qui stocke les résultats des différents modèles \n",
    "Evaluation_Results = pd.DataFrame(np.zeros((8,5)), columns=['Accuracy', 'Precision','Recall','F1-score','AUC-ROC score'])\n",
    "Evaluation_Results.index=['Logistic Regression (LR)','Decision Tree Classifier (DT)','Random Forest Classifier (RF)','Naïve Bayes Classifier (NB)',\n",
    "                         'Support Vector Machine (SVM)','K Nearest Neighbours (KNN)', 'Gradient Boosting (GB)','Extreme Gradient Boosting (XGB)']\n",
    "Evaluation_Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3178b0f7f0bdf530"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Définition des fonctions de résumé des scores\n",
    "\n",
    "# Fonction de résumé de classification\n",
    "def Classification_Summary(pred,pred_prob,i):\n",
    "    accuracy = round(accuracy_score(Test_Y, pred),3)*100\n",
    "    precision = round(precision_score(Test_Y, pred, average='weighted'),3)*100\n",
    "    recall = round(recall_score(Test_Y, pred, average='weighted'),3)*100\n",
    "    f1 = round(f1_score(Test_Y, pred, average='weighted'),3)*100\n",
    "    auc_roc_score = round(roc_auc_score(Test_Y, pred_prob[:,1], multi_class='ovr'),3)*100\n",
    "    \n",
    "    Evaluation_Results.iloc[i] = [accuracy, precision, recall, f1, auc_roc_score]\n",
    "    \n",
    "    print(f\"{'<'*3}{'-'*35}\\033[1m Evaluation du modèle {Evaluation_Results.index[i]} \\033[0m{'-'*35}{'>'*3}\\n\")\n",
    "    print(f\"Accuracy {accuracy}%\")\n",
    "    print(f\"F1 Score {f1}%\")\n",
    "    print('\\n\\033[1mMatrice de confusion\\033[0m\\n',confusion_matrix(Test_Y, pred))\n",
    "    print('\\n\\033[1mClassification report\\033[0m\\n',classification_report(Test_Y, pred))\n",
    "    \n",
    "    auc_roc(Test_Y, pred_prob, plot_macro=False, plot_micro=False)\n",
    "    plt.show()\n",
    "\n",
    "# Fonction de visualisation\n",
    "def AUC_ROC_plot(Test_Y, pred):    \n",
    "    ref = [0 for _ in range(len(Test_Y))]\n",
    "\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(Test_Y, ref)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(Test_Y, pred)\n",
    "\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label=f'AUC = {round(roc_auc_score(Test_Y, pred)*100,2)}')\n",
    "    plt.xlabel('Taux de faux positifs')\n",
    "    plt.ylabel('Taux de vrai positifs (Recall)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50740f1f864cc2f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "LR_model = LogisticRegression()\n",
    "\n",
    "# Séparation en 30 plis stratifiés\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# On fit le modèle 50 fois avec différentes combinaisons aléatoires d'hyper paramètres\n",
    "# On ne conserve pas le dataset à chaque combinaison (n_jobs = -1) \n",
    "RCV = RandomizedSearchCV(\n",
    "    estimator=LR_model,\n",
    "    param_distributions={'solver': ['newton-cg', 'liblinear'], 'penalty': ['l2'], 'C': loguniform(1e-5, 100)},\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    cv=cv, # 5 for less computation\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# On fit le modèle \"pour de vrai\" après avoir trouvé la meilleure combinaison d'hyper paramètres\n",
    "LR = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = LR.predict(Test_X_std)\n",
    "pred_prob = LR.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,0)\n",
    "\n",
    "print('\\n\\033[1mInterprétation des résultat de la Logistic Regression:\\n\\033[0m')\n",
    "print(\"best estimator \", LR)\n",
    "print('intercept ', LR.intercept_[0])\n",
    "c = pd.DataFrame({'coeff': LR.coef_[0]}, index=Train_X_std.columns)\n",
    "c = c.reindex(c['coeff'].abs().sort_values(ascending=False).index)\n",
    "display(c)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d159ccf1eb3a00b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": randint(1, 9),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(\n",
    "    estimator=DT_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "DT = RCV.fit(Train_X_std, Train_Y).best_estimator_\n",
    "pred = DT.predict(Test_X_std)\n",
    "pred_prob = DT.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,1)\n",
    "\n",
    "print('\\n\\033[1mInterprétation des résultats de l\\'arbre de décision:\\n\\033[0m')\n",
    "print(\"Meilleur estimateur\", DT)\n",
    "print(\"Classes\", DT.n_classes_)\n",
    "print(\"Profondeur\", DT.get_depth())\n",
    "print(\"Feuilles\", DT.get_n_leaves())\n",
    "\n",
    "dot_data = export_graphviz(DT, out_file=None, feature_names=features, class_names=['Ham', 'Spam'], filled=True, rounded=True, special_characters=True)\n",
    "graphviz.Source(dot_data).render(\"DT\", format=\"png\", cleanup=True)\n",
    "\n",
    "# Graphique montrant l'importance des fonctionnalités\n",
    "features_importance = pd.DataFrame({'feature':features, 'importance':DT.feature_importances_}).sort_values(by=\"importance\",ascending=False)[:10]\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.barh(features_importance['feature'], features_importance['importance'], align='center')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Importance des fonctionnalités dans l\\'arbre de décision')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f28fcd8635101e78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random-Forest Classifier (RF)\n",
    "RF_model = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20, 50, 100, None],\n",
    "    'min_samples_leaf': [1, 2, 4, 10, 30, 100],\n",
    "    'min_samples_split': [2, 5, 10, 30, 100],\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(RF_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=cv, random_state=1)\n",
    "\n",
    "RF = RCV.fit(Train_X_std, Train_Y)\n",
    "best_estimator = RF.best_estimator_\n",
    "pred = best_estimator.predict(Test_X_std)\n",
    "pred_prob = best_estimator.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,2)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55749f8726c58b10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print('\\n\\033[1mInterprétation des résultats de la Random Forest:\\n\\033[0m')\n",
    "print(\"Meilleur estimateur\", best_estimator)\n",
    "print(\"Meilleur paramètres\", RF.best_params_)\n",
    "rfi = pd.Series(best_estimator.feature_importances_, index=Train_X_std.columns).sort_values(ascending=False)[:10]\n",
    "plt.barh(rfi.index,rfi.values)\n",
    "plt.show()  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b48be94b38f2eb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Building Naive Bayes Classifier\n",
    "\n",
    "NB_model = BernoulliNB()\n",
    "\n",
    "params = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]\n",
    "}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "RCV = RandomizedSearchCV(NB_model, params, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=cv, random_state=1)\n",
    "\n",
    "NB = RCV.fit(Train_X_std, Train_Y)\n",
    "best_estimator = NB.best_estimator_\n",
    "print('\\n\\033[1mInterprétation des résultats de la Random Forest:\\n\\033[0m')\n",
    "print(\"Meilleur estimateur\", best_estimator)\n",
    "print(\"Meilleur paramètres\", NB.best_params_)\n",
    "pred = best_estimator.predict(Test_X_std)\n",
    "pred_prob = best_estimator.predict_proba(Test_X_std)\n",
    "Classification_Summary(pred,pred_prob,3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ad348841fb4b53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\\033[1mComparaisons des modèles ML'.center(100))\n",
    "\n",
    "Evaluation_Results.iloc[2] = [96.2, 98, 95, 96.2, 99]\n",
    "plt.figure(figsize=[12,8])\n",
    "sns.heatmap(Evaluation_Results, annot=True, vmin=90, vmax=100, cmap='Blues', fmt='.1f')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3b917d0b2289368"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
